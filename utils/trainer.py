import os
import copy
import random
import time
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import torch
import torch.nn as nn
from torch.amp import GradScaler, autocast
from torch.optim import Optimizer
from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler
from torch.utils.data import DataLoader
from typing import Any, Generator, Iterator, Optional, Dict, List, Union, Tuple
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

def plot_confusion_matrix(y_true, y_pred, class_names=None):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶è¿”å› matplotlib figure å¯¹è±¡ã€‚
    """
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(10, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names if class_names else "auto",
                yticklabels=class_names if class_names else "auto")
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.tight_layout()
    return fig

def plot_roc_curve(y_true, y_score, class_names=None):
    """
    ç»˜åˆ¶ ROC æ›²çº¿å¹¶è¿”å› matplotlib figure å¯¹è±¡ã€‚
    æ”¯æŒäºŒåˆ†ç±»å’Œå¤šåˆ†ç±» (One-vs-Rest)ã€‚
    """
    n_classes = len(class_names) if class_names else (y_score.shape[1] if y_score.ndim > 1 else 2)
    
    fig, ax = plt.subplots(figsize=(10, 10))
    
    # äºŒåˆ†ç±»æƒ…å†µ (y_score shape [N, 1] or [N])
    if n_classes == 2 or (y_score.ndim == 1) or (y_score.shape[1] == 1):
        # å‡è®¾ y_score æ˜¯æ­£ç±»çš„æ¦‚ç‡
        # å¦‚æœ y_score æ˜¯ [N, 1]ï¼Œsqueeze
        if y_score.ndim == 2: y_score = y_score.squeeze()
        
        fpr, tpr, _ = roc_curve(y_true, y_score)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    else:
        # å¤šåˆ†ç±»æƒ…å†µ
        # éœ€è¦å°† y_true äºŒå€¼åŒ–
        y_true_bin = label_binarize(y_true, classes=range(n_classes))
        
        # è®¡ç®—æ¯ä¸€ç±»çš„ ROC
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
            
        # ç»˜åˆ¶å¾®å¹³å‡ ROC æ›²çº¿ (Micro-average)
        fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
        
        plt.plot(fpr["micro"], tpr["micro"],
                 label=f'micro-average ROC curve (area = {roc_auc["micro"]:.2f})',
                 color='deeppink', linestyle=':', linewidth=4)

        # ç»˜åˆ¶æ¯ä¸€ç±»çš„ ROC æ›²çº¿
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])
        for i, color in zip(range(n_classes), colors):
            label = f'ROC curve of class {class_names[i] if class_names else i}'
            label += f' (area = {roc_auc[i]:.2f})'
            plt.plot(fpr[i], tpr[i], color=color, lw=2, label=label)

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.tight_layout()
    return fig

def check_sanity(loss: torch.Tensor, step: int):
    if torch.isnan(loss) or torch.isinf(loss):
        print(f"CRITICAL WARNING: Loss became NaN or Inf at step {step}!")
        return False
    return True

def set_seed(seed: int = 42):
    """
    å›ºå®šéšæœºç§å­ä»¥ä¿è¯å®éªŒçš„å¯å¤ç°æ€§ã€‚
    
    åŒ…æ‹¬ random, numpy, torch ä»¥åŠ cuda çš„ç§å­è®¾ç½®ã€‚
    æ³¨æ„ï¼šè®¾ç½® cudnn.deterministic = True å¯èƒ½ä¼šé™ä½è®­ç»ƒé€Ÿåº¦ã€‚

    Args:
        seed (int): éšæœºç§å­æ•°å€¼ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # ç¡®ä¿ CUDA é€‰æ‹©ç¡®å®šæ€§ç®—æ³•
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"Random seed set to {seed}")

def match_shape_if_needed(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    æ£€æŸ¥å¼ é‡ a æ˜¯å¦éœ€è¦è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é… bã€‚
    
    å¸¸ç”¨äºå¤„ç† Binary Cross Entropy ä¸­é¢„æµ‹å€¼ [N, 1] ä¸ ç›®æ ‡å€¼ [N] ä¸åŒ¹é…çš„æƒ…å†µã€‚

    Args:
        a (torch.Tensor): é¢„æµ‹å¼ é‡æˆ–æºå¼ é‡ã€‚
        b (torch.Tensor): ç›®æ ‡å¼ é‡ã€‚

    Returns:
        torch.Tensor: è°ƒæ•´å½¢çŠ¶åçš„å¼ é‡ aã€‚
    """
    if a.dim() == 2 and b.dim() == 1 and a.shape[0] == 1 and a.shape[1] == b.shape[0]:
        return a.squeeze(0)
    if a.dim() == 2 and b.dim() == 1 and a.shape[1] == 1 and a.shape[0] == b.shape[0]:
         return a.squeeze(1)
    return a

class TimingContext:
    def __init__(self, name="Block"):
        self.name = name

    def __enter__(self):
        self.start = time.time()
        torch.cuda.synchronize() # å¦‚æœç”¨ GPUï¼Œå¿…é¡»åŒæ­¥æ‰èƒ½æµ‹å‡†
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        torch.cuda.synchronize()
        print(f"[{self.name}] elapsed: {(time.time() - self.start)*1000:.2f} ms")

class Timer:
    def __init__(self):
        self.start_time = time.time()
        self.epoch_start_time = 0
    
    def start_epoch(self):
        self.epoch_start_time = time.time()
        
    def end_epoch(self) -> str:
        elapsed = time.time() - self.epoch_start_time
        return str(datetime.timedelta(seconds=int(elapsed)))
    
    def total_time(self) -> str:
        elapsed = time.time() - self.start_time
        return str(datetime.timedelta(seconds=int(elapsed)))

class ModelEMA:
    """
    æ¨¡å‹æƒé‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ (Exponential Moving Average)ã€‚
    """
    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.decay = decay
        # åˆ›å»ºå½±å­æ¨¡å‹ (Shadow Model)ï¼Œåªä¿å­˜æƒé‡ï¼Œä¸å‚ä¸åå‘ä¼ æ’­
        self.shadow = copy.deepcopy(model.module if hasattr(model, 'module') else model)
        self.shadow.eval()
        # å°†å‚æ•°è®¾ä¸ºä¸éœ€è¦æ¢¯åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
        for param in self.shadow.parameters():
            param.requires_grad = False
    def update(self, model: nn.Module):
        """æ ¹æ®å½“å‰æ¨¡å‹å‚æ•°æ›´æ–°å½±å­å‚æ•°"""
        # å…¼å®¹ DataParallel
        model_to_use = model.module if hasattr(model, 'module') else model
        
        with torch.no_grad():
            msd = model_to_use.state_dict()
            for name, param in self.shadow.named_parameters():
                if name in msd:
                    new_param = msd[name]
                    # ç¡®ä¿è®¾å¤‡ä¸€è‡´
                    if param.device != new_param.device:
                         param.data = param.data.to(new_param.device)
                    # EMA å…¬å¼: shadow = decay * shadow + (1 - decay) * new_param
                    param.data.mul_(self.decay).add_(new_param.data, alpha=1 - self.decay)
    def apply_shadow(self, model: nn.Module):
        """å°† EMA æƒé‡èµ‹å€¼ç»™åŸæ¨¡å‹ (é€šå¸¸åœ¨æ¨ç†æˆ–ä¿å­˜æœ€ä½³æ¨¡å‹å‰ä½¿ç”¨)"""
        model_to_use = model.module if hasattr(model, 'module') else model
        model_to_use.load_state_dict(self.shadow.state_dict())

class Trainer:
    """
    ä¸€ä¸ªå…¨åŠŸèƒ½çš„ PyTorch è®­ç»ƒ/è¯„ä¼°è¿­ä»£å™¨å°è£…ç±»ã€‚
    è¯¥ç±»æ—¨åœ¨é€šè¿‡ç”Ÿæˆå™¨æ¨¡å¼ (Generator Pattern) ç®€åŒ–è®­ç»ƒå¾ªç¯ï¼ŒåŒæ—¶ä¿ç•™æé«˜çš„çµæ´»æ€§ã€‚
    
    ä¸»è¦ç‰¹æ€§:
    - **è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)**: æ”¯æŒ fp16 è®­ç»ƒã€‚
    - **æ¢¯åº¦ç­–ç•¥**: æ”¯æŒæ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation) å’Œæ¢¯åº¦è£å‰ª (Gradient Clipping)ã€‚
    - **ç”Ÿå‘½å‘¨æœŸç®¡ç†**: æ”¯æŒæ–­ç‚¹ç»­è®­ (Resume)ã€æ¨¡å‹ä¿å­˜ (Checkpointing)ã€æ—©åœ (Early Stopping)ã€‚
    - **å¯è§†åŒ–**: é›†æˆ TensorBoard æ—¥å¿—è®°å½•ã€‚
    - **æ˜“ç”¨æ€§**: è‡ªåŠ¨å¤„ç†è®¾å¤‡ç§»åŠ¨ã€è¿›åº¦æ¡æ˜¾ç¤º (tqdm) å’ŒæŒ‡æ ‡è®¡ç®—ã€‚
    - **é«˜çº§åŠŸèƒ½**: æ”¯æŒ EMAã€DataParallelã€æ¨ç†é¢„æµ‹ (Predict)ã€‚
    Attributes:
        model (nn.Module): æ­£åœ¨è®­ç»ƒçš„æ¨¡å‹ã€‚
        device (torch.device): å½“å‰è¿è¡Œè®¾å¤‡ã€‚
        history (Dict): è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss å’Œ metric å†å²ã€‚
        global_step (int): å…¨å±€è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨ã€‚
    """

    def __init__(
        self,
        model: nn.Module,
        num_epochs: int,
        train_loader: Optional[DataLoader] = None,
        test_loader: Optional[DataLoader] = None,
        optimizer: Optional[Optimizer] = None,
        criterion: Optional[nn.Module] = None,
        scheduler: Optional[_LRScheduler] = None,
        checkpoint_path: Optional[str] = None,
        device: Optional[torch.device] = None,
        use_amp: bool = False,
        accumulation_steps: int = 1,
        grad_clip_norm: Optional[float] = None,
        use_ema: bool = False,
        ema_decay: float = 0.999
    ) -> None:
        """
        åˆå§‹åŒ– Trainer å®ä¾‹ã€‚

        Args:
            model (nn.Module): å¾…è®­ç»ƒçš„ PyTorch æ¨¡å‹ã€‚
            num_epochs (int): è®­ç»ƒçš„æ€» Epoch æ•°ã€‚
            train_loader (DataLoader, optional): è®­ç»ƒæ•°æ®åŠ è½½å™¨ã€‚
            test_loader (DataLoader, optional): éªŒè¯/æµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚
            optimizer (Optimizer, optional): ä¼˜åŒ–å™¨å®ä¾‹ã€‚
            criterion (nn.Module, optional): æŸå¤±å‡½æ•°å®ä¾‹ã€‚
            scheduler (_LRScheduler, optional): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
            checkpoint_path (str, optional): æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„ (ä¾‹å¦‚ 'checkpoints/ckpt.pt')ã€‚
            device (torch.device, optional): æŒ‡å®šè¿è¡Œè®¾å¤‡ã€‚è‹¥ä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹ CUDA/CPUã€‚
            use_amp (bool): æ˜¯å¦å¼€å¯è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ (éœ€è¦ GPU æ”¯æŒ)ã€‚
            accumulation_steps (int): æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤ä¸º 1 (ä¸ç´¯ç§¯)ã€‚
            grad_clip_norm (float, optional): æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°ã€‚None è¡¨ç¤ºä¸è£å‰ªã€‚
        """
        self.model = model
        self.num_epochs = num_epochs
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.optimizer = optimizer
        self.criterion = criterion
        self.scheduler = scheduler
        self.checkpoint_path = checkpoint_path
        
        # --- é«˜çº§è®­ç»ƒé…ç½® ---
        self.use_amp = use_amp
        self.scaler = GradScaler() if use_amp else None
        self.accumulation_steps = accumulation_steps
        self.grad_clip_norm = grad_clip_norm

        # --- è®¾å¤‡ç®¡ç†ä¸ DataParallel ---
        # é€»è¾‘é¡ºåºï¼šç¡®å®š Device -> ç§»åŠ¨æ¨¡å‹ -> åŒ…è£… DataParallel
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
            
        # å…ˆå°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ (è¿™å¯¹ DataParallel å¾ˆé‡è¦)
        self.model.to(self.device)
        
        # è‡ªåŠ¨æ£€æµ‹å¹¶åº”ç”¨ DP (æ— è®º device æ˜¯æ‰‹åŠ¨ä¼ å…¥è¿˜æ˜¯è‡ªåŠ¨æ£€æµ‹)
        self.use_dp = False
        self._try_init_dataparallel()

        # --- åˆå§‹åŒ– EMA ---
        self.use_ema = use_ema
        self.ema = ModelEMA(self.model, decay=ema_decay) if use_ema else None
        if self.use_ema:
            print(f"EMA enabled with decay {ema_decay}")

        # --- çŠ¶æ€å˜é‡ ---
        self.epoch: int = 0
        self.start_epoch: int = 0
        self.batch_idx: int = 0
        self.global_step: int = 0  # è®°å½•æ€»çš„ optimizer step æ¬¡æ•° (batchæ•°)
        
        # å½“å‰ Batch çš„æ•°æ®
        self.data: Optional[Union[torch.Tensor, Tuple[torch.Tensor]]] = None
        self.target: Optional[torch.Tensor] = None
        
        # å¾ªç¯æ§åˆ¶æ ‡å¿—
        self.is_first_batch_in_epoch: bool = False
        self.is_last_batch_in_epoch: bool = False
        
        # ç»Ÿè®¡å˜é‡
        self.loss: Optional[torch.Tensor] = None # æœ€è¿‘ä¸€æ¬¡ forward çš„ loss
        self.running_loss: float = 0.0 # å½“å‰ epoch ç´¯è®¡ loss
        self.running_samples: int = 0  # å½“å‰ epoch ç´¯è®¡æ ·æœ¬æ•°
        
        # --- è¯„ä¼°ç»Ÿè®¡ ---
        self.eval_loss: float = 0.0
        self.correct_predictions: int = 0
        self.total_predictions: int = 0
        self.timer = Timer()

        # --- å†å²è®°å½• ---
        self.history: Dict[str, List[float]] = {
            'train_loss': [],
            'val_loss': [],
            'val_acc': []
        }
        self.best_val_metric = -float('inf') # ç”¨äºä¿å­˜ best model

        # --- TensorBoard & Early Stopping ---
        self.writer = None  # TensorBoard writer
        self.patience_counter: int = 0 # æ—©åœè®¡æ•°å™¨
        self.best_metric_for_es: Optional[float] = None # ç”¨äºæ—©åœçš„æœ€ä½³æŒ‡æ ‡

        # --- æ··æ·†çŸ©é˜µç›¸å…³ ---
        self.classes: Optional[Union[List, Tuple]] = None
        self.top_k: Optional[int] = None
        self.y_trues: List[torch.Tensor] = []
        self.y_preds: List[torch.Tensor] = []
        self.y_scores: List[torch.Tensor] = [] # ç”¨äº ROC æ›²çº¿
        self.correct_top_k_predictions: int = 0
        self.enable_confusion_matrix: bool = False
        self.enable_roc_curve: bool = False

        # --- åˆå§‹åŒ– ---
        self._display_model_summary()
        if self.checkpoint_path is not None:
            # å°è¯•è‡ªåŠ¨åŠ è½½ 'last.pt' æˆ–æŒ‡å®šè·¯å¾„
            self.load_checkpoint()

    def init_classes(self, classes: Union[List, Tuple], top_k: Optional[int] = None, 
                     force_confusion_matrix: bool = False, force_roc_curve: bool = False):
        """
        åˆå§‹åŒ–ç±»åˆ«åç§°åˆ—è¡¨ï¼Œç”¨äºç»˜åˆ¶æ··æ·†çŸ©é˜µå’Œ ROC æ›²çº¿ã€‚
        
        Args:
            classes: ç±»åˆ«åç§°åˆ—è¡¨ã€‚
            top_k: å¦‚æœæŒ‡å®šï¼Œå°†åœ¨è¯„ä¼°æ—¶è®¡ç®— Top-k Accuracyã€‚
            force_confusion_matrix: æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆæ··æ·†çŸ©é˜µ (å³ä½¿ç±»åˆ«æ•°å¾ˆå¤š)ã€‚
            force_roc_curve: æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆ ROC æ›²çº¿ (å³ä½¿ç±»åˆ«æ•°å¾ˆå¤š)ã€‚
        """
        self.classes = classes
        self.top_k = top_k
        
        num_classes = len(classes)
        
        # æ™ºèƒ½åˆ¤æ–­æ˜¯å¦å¼€å¯æ··æ·†çŸ©é˜µ (é»˜è®¤é˜ˆå€¼ 50)
        if num_classes <= 50 or force_confusion_matrix:
            self.enable_confusion_matrix = True
        else:
            self.enable_confusion_matrix = False
            print(f"Confusion Matrix disabled due to large number of classes ({num_classes} > 50). Use force_confusion_matrix=True to override.")

        # æ™ºèƒ½åˆ¤æ–­æ˜¯å¦å¼€å¯ ROC æ›²çº¿ (é»˜è®¤é˜ˆå€¼ 10)
        if num_classes <= 10 or force_roc_curve:
            self.enable_roc_curve = True
        else:
            self.enable_roc_curve = False
            print(f"ROC Curve disabled due to large number of classes ({num_classes} > 10). Use force_roc_curve=True to override.")

    @property
    def display_epoch(self) -> int:
        """è·å–å½“å‰ç”¨äºæ˜¾ç¤ºçš„ Epoch åºå· (ä»1å¼€å§‹è®¡æ•°)ã€‚"""
        return max(self.epoch, self.start_epoch) + 1
    
    @property
    def epoch_mean_loss(self) -> float:
        """è¿”å›å½“å‰ Epoch åˆ°ç›®å‰ä¸ºæ­¢çš„å¹³å‡ Lossã€‚"""
        if self.running_samples == 0:
            return 0.0
        return self.running_loss / self.running_samples
    
    @property
    def eval_accuracy(self) -> float:
        """è¿”å›å½“å‰è¯„ä¼°é˜¶æ®µçš„ç´¯ç§¯å‡†ç¡®ç‡ã€‚"""
        if self.total_predictions == 0:
            return 0.0
        return self.correct_predictions / self.total_predictions
    
    def _try_init_dataparallel(self):
        """å°è¯•åˆå§‹åŒ– DataParallel"""
        if self.device.type == 'cuda' and torch.cuda.device_count() > 1:
            print(f"Using {torch.cuda.device_count()} GPUs with DataParallel!")
            self.model = nn.DataParallel(self.model)
            self.use_dp = True
        else:
            self.use_dp = False

    def _display_model_summary(self):
        """
        æ‰“å°ä¸°å¯Œçš„æ¨¡å‹ç»“æ„ã€ç¯å¢ƒä¿¡æ¯åŠå‚æ•°ç»Ÿè®¡ã€‚
        æ”¯æŒè‡ªåŠ¨è°ƒç”¨ torchinfo (å¦‚æœå·²å®‰è£…)ã€‚
        """
        import sys
        
        # 1. è·å–å®é™…æ¨¡å‹ (å¤„ç† DataParallel)
        real_model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # 2. åŸºç¡€ç»Ÿè®¡
        total_params = sum(p.numel() for p in real_model.parameters())
        trainable_params = sum(p.numel() for p in real_model.parameters() if p.requires_grad)
        # ä¼°ç®—æ¨¡å‹æƒé‡å ç”¨çš„æ˜¾å­˜ (Float32 = 4 bytes)
        # æ³¨æ„ï¼šè¿™åªæ˜¯é™æ€æƒé‡ï¼Œä¸åŒ…å«ä¸­é—´æ¿€æ´»å€¼å’Œæ¢¯åº¦
        param_memory_mb = total_params * 4 / (1024 ** 2) 
        
        # 3. æ ¼å¼åŒ–æ‰“å°
        print("=" * 80)
        print(f"ğŸŸ¢ SYSTEM & ENV SUMMARY")
        print("-" * 80)
        print(f"{'PyTorch Version':<20} : {torch.__version__}")
        print(f"{'Python Version':<20} : {sys.version.split()[0]}")
        print(f"{'Device':<20} : {self.device}")
        
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(self.device)
            print(f"{'GPU Name':<20} : {gpu_name}")
            print(f"{'CUDA Version':<20} : {torch.version.cuda}")
            if hasattr(self, 'use_dp') and self.use_dp:
                 print(f"{'Distributed':<20} : DataParallel (GPUs: {torch.cuda.device_count()})")
        
        print("-" * 80)
        print(f"ğŸ”µ TRAINING CONFIG")
        print("-" * 80)
        print(f"{'AMP (Mixed Precision)':<25} : {'ON' if self.use_amp else 'OFF'}")
        print(f"{'Gradient Accumulation':<25} : {self.accumulation_steps} steps")
        print(f"{'Gradient Clipping':<25} : {self.grad_clip_norm if self.grad_clip_norm else 'OFF'}")
        print(f"{'Optimizer':<25} : {self.optimizer.__class__.__name__ if self.optimizer else 'None'}")
        if self.optimizer:
            try:
                lr = self.optimizer.param_groups[0]['lr']
                print(f"{'Initial Learning Rate':<25} : {lr}")
            except: pass
        print(f"{'EMA (Exp Moving Avg)':<25} : {'ON' if (hasattr(self, 'use_ema') and self.use_ema) else 'OFF'}")

        print("-" * 80)
        print(f"ğŸŸ¡ MODEL SUMMARY: {real_model.__class__.__name__}")
        print("-" * 80)
        print(f"{'Layer (type)':<30} | {'Params':>12} | {'Trainable':>10}")
        print("-" * 60)
        
        for name, module in real_model.named_children():
            # è®¡ç®—å­æ¨¡å—å‚æ•°
            mod_params = sum(p.numel() for p in module.parameters())
            mod_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            is_train = "Yes" if mod_trainable > 0 else "No"
            name_str = f"{name} ({module.__class__.__name__})"
            # æˆªæ–­è¿‡é•¿çš„åå­—
            if len(name_str) > 28: name_str = name_str[:25] + "..."
            
            print(f"{name_str:<30} | {mod_params:>12,} | {is_train:>10}")
        
        print("-" * 60)
        print(f"{'Total Params':<30} : {total_params:,}")
        print(f"{'Trainable Params':<30} : {trainable_params:,} ({trainable_params/total_params:.1%})")
        print(f"{'Non-Trainable Params':<30} : {total_params - trainable_params:,}")
        print(f"{'Est. Model Size (Weights)':<30} : {param_memory_mb:.2f} MB")
        
        print("=" * 80)

    def init_tensorboard(self, log_dir: str = "runs") -> 'Trainer':
        """
        åˆå§‹åŒ– TensorBoard SummaryWriterã€‚
        å¦‚æœ torch.utils.tensorboard æœªå®‰è£…ï¼Œåˆ™ä¸åšä»»ä½•æ“ä½œã€‚

        Args:
            log_dir (str): æ—¥å¿—ä¿å­˜ç›®å½•ã€‚
        
        Returns:
            self (Trainer): è¿”å›å½“å‰ Trainer å¯¹è±¡ï¼Œä»¥ä¾¿é“¾å¼è°ƒç”¨ã€‚
        """
        try:
            from torch.utils.tensorboard import SummaryWriter
            
            # è·å–æ¨¡å‹åç§°
            real_model = self.model.module if hasattr(self.model, 'module') else self.model
            model_name = real_model.__class__.__name__
            
            # å¦‚æœç”¨æˆ·ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼Œè‡ªåŠ¨æ·»åŠ æ¨¡å‹åå’Œæ—¶é—´æˆ³
            if log_dir == "runs":
                timestamp = datetime.datetime.now().strftime("%b%d_%H-%M-%S")
                log_dir = os.path.join("runs", f"{model_name}_{timestamp}")
            
            self.writer = SummaryWriter(log_dir=log_dir)
            
            # è®°å½•æ¨¡å‹åç§°
            self.writer.add_text("Model/Name", model_name)
            
            print(f"TensorBoard initialized. Logs will be saved to: {log_dir}")
        except ImportError:
            print("Warning: TensorBoard not found. Install it using 'pip install tensorboard'.")
        finally:
            return self

    def log(self, metrics: Dict[str, float], step: Optional[int] = None):
        """
        æ‰‹åŠ¨è®°å½•æŒ‡æ ‡åˆ° TensorBoardã€‚

        Args:
            metrics (Dict[str, float]): æŒ‡æ ‡å­—å…¸ï¼Œå¦‚ {'Val/Loss': 0.5, 'Val/Acc': 0.9}ã€‚
            step (int, optional): å½“å‰æ­¥æ•°ã€‚å¦‚æœä¸å¡«ï¼Œé»˜è®¤ä½¿ç”¨ self.global_stepã€‚
        """
        if self.writer is None:
            return
        
        step_to_use = step if step is not None else self.global_step
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, step_to_use)
    
    def log_confusion_matrix(self, loader, class_names=None):
        if self.writer is None: return
        preds, targets = self.predict(loader, return_targets=True)
        # è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•
        if preds.ndim > 1: preds = preds.argmax(dim=1)
        if targets.ndim > 1: targets = targets.argmax(dim=1)
        
        fig = plot_confusion_matrix(targets.numpy(), preds.numpy(), class_names)
        self.writer.add_figure("Eval/Confusion_Matrix", fig, self.global_step)
        plt.close(fig)

    def check_early_stopping(self, current_metric: float, monitor: str = 'val_loss', patience: int = 5) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è§¦å‘æ—©åœ (Early Stopping)ã€‚

        Args:
            current_metric (float): å½“å‰ epoch çš„éªŒè¯æŒ‡æ ‡å€¼ã€‚
            monitor (str): ç›‘æ§æŒ‡æ ‡åç§° ('val_loss' æˆ– 'val_acc')ï¼Œç”¨äºå†³å®šæ˜¯ 'min' è¿˜æ˜¯ 'max' æ¨¡å¼ã€‚
                           åŒ…å« 'loss' è§†ä¸ºè¶Šå°è¶Šå¥½ï¼Œå¦åˆ™è§†ä¸ºè¶Šå¤§è¶Šå¥½ã€‚
            patience (int): å®¹å¿å¤šå°‘ä¸ª epoch æŒ‡æ ‡æœªæ”¹å–„ã€‚

        Returns:
            bool: å¦‚æœè¿”å› Trueï¼Œåˆ™åº”å½“åœæ­¢è®­ç»ƒå¾ªç¯ã€‚
        """
        # é¦–æ¬¡è°ƒç”¨åˆå§‹åŒ–
        if self.best_metric_for_es is None:
             self.best_metric_for_es = float('inf') if 'loss' in monitor.lower() else -float('inf')

        is_better = False
        if 'loss' in monitor.lower():
            if current_metric < self.best_metric_for_es:
                is_better = True
        else:
            if current_metric > self.best_metric_for_es:
                is_better = True

        if is_better:
            self.best_metric_for_es = current_metric
            self.patience_counter = 0
        else:
            self.patience_counter += 1
            # åªæœ‰åœ¨è®¡æ•°å™¨å¢åŠ æ—¶æ‰æ‰“å°
            if self.patience_counter > 0:
                print(f"Early Stopping Counter: {self.patience_counter}/{patience}")

        if self.patience_counter >= patience:
            print(f"Early stopping triggered after {patience} epochs without improvement.")
            return True
        
        return False

    def find_lr(self, train_loader: DataLoader = None, init_value: float = 1e-8, final_value: float = 10.0, beta: float = 0.98) -> None:
        """
        æ¨¡æ‹Ÿè®­ç»ƒä»¥å¯»æ‰¾æœ€ä½³å­¦ä¹ ç‡ã€‚ä¼šç»˜åˆ¶ Loss vs LR æ›²çº¿å¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚
        æ³¨æ„ï¼šè¿è¡Œæ­¤æ–¹æ³•åä¼šé‡ç½®æ¨¡å‹å‚æ•°åˆ°è¿è¡Œå‰çŠ¶æ€ã€‚
        """
        train_loader = train_loader if train_loader else self.train_loader
        if train_loader is None:
            raise ValueError("No train_loader provided.")
        print("Finding learning rate...")
        # 1. ä¿å­˜å½“å‰çŠ¶æ€ä»¥æ¢å¤
        if isinstance(self.model, nn.DataParallel):
            model_state = copy.deepcopy(self.model.module.state_dict())
        else:
            model_state = copy.deepcopy(self.model.state_dict())
        optimizer_state = copy.deepcopy(self.optimizer.state_dict())
        
        self.model.train()
        num = len(train_loader) - 1
        mult = (final_value / init_value) ** (1 / num)
        lr = init_value
        self.optimizer.param_groups[0]['lr'] = lr
        
        avg_loss = 0.0
        best_loss = 0.0
        batch_num = 0
        losses = []
        lrs = []
        
        # ç¦ç”¨ AMP scaler é¿å…å¹²æ‰°ï¼Œæˆ–è€…åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„
        scaler = GradScaler() if self.use_amp else None
        
        try:
            for batch_data in tqdm(train_loader, desc="LR Finder", leave=False):
                batch_num += 1
                self._process_batch_data(batch_data)
                
                # Forward
                with autocast(device_type=self.device.type, enabled=self.use_amp):
                    if isinstance(self.data, tuple):
                        logits = self.model(*self.data)
                    else:
                        logits = self.model(self.data)
                    loss = self.criterion(logits, self.target)
                
                # Compute the smoothed loss
                loss_val = loss.item()
                avg_loss = beta * avg_loss + (1 - beta) * loss_val
                smoothed_loss = avg_loss / (1 - beta**batch_num)
                
                # Stop if the loss is exploding
                if batch_num > 1 and smoothed_loss > 4 * best_loss:
                    break
                if smoothed_loss < best_loss or batch_num == 1:
                    best_loss = smoothed_loss
                
                losses.append(smoothed_loss)
                lrs.append(lr)
                
                # Optimize
                self.optimizer.zero_grad()
                if self.use_amp and scaler:
                    scaler.scale(loss).backward()
                    scaler.step(self.optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()
                
                # Update LR
                lr *= mult
                self.optimizer.param_groups[0]['lr'] = lr
        finally:
            # 2. æ¢å¤æ¨¡å‹çŠ¶æ€
            if isinstance(self.model, nn.DataParallel):
                self.model.module.load_state_dict(model_state)
            else:
                self.model.load_state_dict(model_state)
            self.optimizer.load_state_dict(optimizer_state)
            print("LR Finder finished. Model state restored.")

        # 3. ç»˜å›¾
        plt.figure(figsize=(10, 6))
        plt.plot(lrs, losses)
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Loss')
        plt.title('Learning Rate Finder')
        plt.grid(True, which="both", ls="-", alpha=0.5)
        plt.savefig('lr_finder_result.png')
        print("Result saved to 'lr_finder_result.png'.")

    def _process_batch_data(self, batch_data: Any):
        """
        å†…éƒ¨æ–¹æ³•ï¼šå¤„ç† Batch æ•°æ®ï¼Œå°†å…¶ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡å¹¶æ‹†åˆ†ä¸º data å’Œ targetã€‚
        
        Args:
            batch_data: DataLoader è¿”å›çš„ä¸€ä¸ª batch æ•°æ®ã€‚
        """
        if isinstance(batch_data, (list, tuple)):
            if len(batch_data) > 1:
                # å‡è®¾æœ€åä¸€ä¸ªæ˜¯ targetï¼Œå‰é¢éƒ½æ˜¯ input
                # å¤„ç†å¤šè¾“å…¥çš„æƒ…å†µ
                inputs = tuple(d.to(self.device) for d in batch_data[:-1])
                self.target = batch_data[-1].to(self.device)
                
                # å¦‚æœåªæœ‰ä¸€ä¸ªè¾“å…¥ï¼Œè§£åŒ… tuple
                if len(inputs) == 1:
                    self.data = inputs[0]
                else:
                    self.data = inputs
            else:
                # åªæœ‰æ•°æ®æ²¡æœ‰æ ‡ç­¾ï¼ˆå¦‚æ— ç›‘ç£å­¦ä¹ ï¼‰
                self.data = batch_data[0].to(self.device)
                self.target = None
        else:
            # åªæœ‰ tensor
            self.data = batch_data.to(self.device)
            self.target = None

    def _create_train_iterator(self, data_loader: DataLoader, tqdm_bar: bool, print_loss: bool) -> Generator['Trainer', None, None]:
        """å†…éƒ¨æ–¹æ³•ï¼šç”Ÿæˆè®­ç»ƒå¾ªç¯çš„è¿­ä»£å™¨ã€‚"""
        self.model.train()
        num_batches = len(data_loader)
        
        for epoch_num in range(self.start_epoch, self.num_epochs):
            self.epoch = epoch_num
            self.running_loss = 0.0
            self.running_samples = 0
            
            iterable = tqdm(data_loader, desc=f"Train Ep {self.display_epoch}/{self.num_epochs}", leave=False) if tqdm_bar else data_loader

            self.timer.start_epoch()
            for batch_idx, batch_data in enumerate(iterable):
                self.batch_idx = batch_idx
                self.global_step += 1
                self._process_batch_data(batch_data)
                
                self.is_first_batch_in_epoch = (batch_idx == 0)
                self.is_last_batch_in_epoch = (batch_idx == num_batches - 1)
                
                # Yield self allowing external control loop
                yield self
            
            # Epoch ç»“æŸè®°å½•
            epoch_time = self.timer.end_epoch()
            epoch_loss = self.epoch_mean_loss
            self.history['train_loss'].append(epoch_loss)
            
            # Log epoch loss to TensorBoard
            self.log({'Train/Epoch_Loss': epoch_loss}, step=self.display_epoch)
            
            if print_loss:
                print(f"Epoch {self.display_epoch} finished in {epoch_time}. Avg Loss = {epoch_loss:.6f}")
            
            # å¯ä»¥åœ¨è¿™é‡ŒåŠ å…¥ scheduler step (epochçº§)
            if self.scheduler is not None and not isinstance(self.scheduler, ReduceLROnPlateau):
                # ç®€å•çš„ epoch stepï¼Œå¦‚æœéœ€è¦ metric step éœ€åœ¨å¤–éƒ¨è°ƒç”¨ auto_step_scheduler
                if not hasattr(self.scheduler, 'step_batch'): # æ’é™¤ warmup ç­‰ batch çº§ scheduler
                     self.scheduler.step()

    def train(self, train_loader: Optional[DataLoader] = None, tqdm_bar: bool = True, print_loss: bool = True) -> Iterator['Trainer']:
        """
        åˆ›å»ºè®­ç»ƒè¿­ä»£å™¨ã€‚
        
        ä½¿ç”¨æ–¹æ³•:
            for trainer in trainer.train():
                loss = trainer.auto_update()
                æˆ–è€…è‡ªå®šä¹‰ update é€»è¾‘

        Args:
            train_loader (DataLoader, optional): è¦†ç›–åˆå§‹åŒ–çš„ DataLoaderã€‚
            tqdm_bar (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ã€‚
            print_loss (bool): æ˜¯å¦åœ¨ Epoch ç»“æŸæ—¶æ‰“å°å¹³å‡ Lossã€‚

        Returns:
            Iterator['Trainer']: äº§ç”Ÿ Trainer å®ä¾‹çš„ç”Ÿæˆå™¨ã€‚
        """
        loader = train_loader if train_loader else self.train_loader
        if not loader:
            raise ValueError("No train_loader provided.")
        return self._create_train_iterator(loader, tqdm_bar, print_loss)

    def _create_eval_iterator(self, data_loader: DataLoader, description: str, tqdm_bar: bool) -> Generator['Trainer', None, None]:
        """å†…éƒ¨æ–¹æ³•ï¼šç”Ÿæˆè¯„ä¼°å¾ªç¯çš„è¿­ä»£å™¨ã€‚"""
        self.model.eval()
        self.eval_loss = 0.0
        self.correct_predictions = 0
        self.correct_top_k_predictions = 0
        self.total_predictions = 0
        num_batches = len(data_loader)
        
        try:
            iterable = tqdm(data_loader, desc=description, leave=False) if tqdm_bar else data_loader
            with torch.no_grad():
                for batch_idx, batch_data in enumerate(iterable):
                    self.batch_idx = batch_idx
                    self.is_first_batch_in_epoch = (batch_idx == 0)
                    self.is_last_batch_in_epoch = (batch_idx == num_batches - 1)
                    self._process_batch_data(batch_data)
                    yield self
        finally:
            # æ¢å¤è®­ç»ƒæ¨¡å¼
            self.model.train()

    def eval(self, test_loader: Optional[DataLoader] = None, description: str = "Evaluating", tqdm_bar: bool = True) -> Iterator['Trainer']:
        """
        åˆ›å»ºè¯„ä¼°è¿­ä»£å™¨ã€‚

        ä½¿ç”¨æ–¹æ³•:
            for trainer in trainer.eval():
                trainer.calculate_classification_metrics()

        Args:
            test_loader (DataLoader, optional): è¦†ç›–åˆå§‹åŒ–çš„ DataLoaderã€‚
            description (str): è¿›åº¦æ¡æè¿°æ–‡å­—ã€‚
            tqdm_bar (bool): æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ã€‚

        Returns:
            Iterator['Trainer']: äº§ç”Ÿ Trainer å®ä¾‹çš„ç”Ÿæˆå™¨ã€‚
        """
        loader = test_loader if test_loader else self.test_loader
        if not loader:
            raise ValueError("No test_loader provided.")
        return self._create_eval_iterator(loader, description, tqdm_bar)

    def update(self, loss: torch.Tensor, step_plateau_with_train_loss: bool = False) -> None:
        """
        æ‰§è¡Œåå‘ä¼ æ’­åŠå‚æ•°æ›´æ–°çš„æ ¸å¿ƒé€»è¾‘ã€‚
        
        åŒ…å«ï¼šæ¢¯åº¦ç¼©æ”¾ (AMP)ã€æ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦è£å‰ªã€ä¼˜åŒ–å™¨æ›´æ–°ã€‚

        Args:
            loss (torch.Tensor): è®¡ç®—å‡ºçš„æŸå¤±å€¼ã€‚
            step_plateau_with_train_loss (bool): æ˜¯å¦ä½¿ç”¨è®­ç»ƒ Loss æ›´æ–° ReduceLROnPlateau è°ƒåº¦å™¨ã€‚
        """
        if self.optimizer is None:
            raise RuntimeError("Optimizer is not set.")
        loss = loss / self.accumulation_steps
        if self.use_amp and self.scaler:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()
        if (self.global_step % self.accumulation_steps == 0) or self.is_last_batch_in_epoch:
            
            # AMP Unscale (ä¸ºäº†èƒ½å¤Ÿæ­£ç¡®è®¡ç®—æ¢¯åº¦èŒƒæ•°å’Œè£å‰ª)
            if self.use_amp and self.scaler:
                self.scaler.unscale_(self.optimizer)
            # è®°å½•æ¢¯åº¦èŒƒæ•° (Gradient Norm) ---
            # å¦‚æœå¯ç”¨äº†è£å‰ªï¼Œclip_grad_norm_ ä¼šè¿”å›åŸå§‹èŒƒæ•°ï¼›
            # å¦‚æœæœªå¯ç”¨è£å‰ªï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è®¡ç®—èŒƒæ•°ç”¨äºè®°å½•ã€‚
            grad_norm = 0.0
            if self.grad_clip_norm is not None:
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)
                grad_norm = grad_norm.item()
            else:
                # æ‰‹åŠ¨è®¡ç®—èŒƒæ•°ç”¨äºæ—¥å¿— (ä¸ä¿®æ”¹æ¢¯åº¦)
                parameters = [p for p in self.model.parameters() if p.grad is not None]
                if parameters:
                    device = parameters[0].grad.device
                    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2).to(device) for p in parameters]), 2)
                    grad_norm = total_norm.item()
            
            # è®°å½•åˆ° TensorBoard
            if self.writer is not None:
                self.writer.add_scalar('Train/Grad_Norm', grad_norm, self.global_step)
            # ä¼˜åŒ–å™¨æ­¥è¿›
            if self.use_amp and self.scaler:
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                self.optimizer.step()
                
            self.optimizer.zero_grad()
            
            # æ›´æ–° EMA
            if self.use_ema and self.ema is not None:
                self.ema.update(self.model)
        
        self.auto_step_scheduler(loss * self.accumulation_steps, step_plateau_with_train_loss)

    def auto_update(self, step_plateau_with_train_loss: bool = False) -> torch.Tensor:
        """
        è‡ªåŠ¨æ‰§è¡Œå®Œæ•´è®­ç»ƒæ­¥ï¼šForward -> Loss -> Backward -> Updateã€‚
        
        å¦‚æœ TensorBoard å·²å¯ç”¨ï¼Œä¼šè‡ªåŠ¨è®°å½• Batch Loss å’Œå­¦ä¹ ç‡ã€‚

        Args:
            step_plateau_with_train_loss (bool): ä¼ é€’ç»™ update æ–¹æ³•ã€‚

        Returns:
            torch.Tensor: å½“å‰ batch çš„åŸå§‹ Loss å€¼ (æœªç»è¿‡ accumulate ç¼©æ”¾)ã€‚
        """
        if not self.optimizer or not self.criterion:
            raise RuntimeError("Optimizer or Criterion missing.")
        
        # Forward & Loss
        with autocast(device_type=self.device.type, enabled=self.use_amp):
            if isinstance(self.data, tuple):
                logits = self.model(*self.data)
            else:
                logits = self.model(self.data)
            
            loss = self.criterion(logits, self.target)

        if not check_sanity(loss, self.global_step):
            raise ValueError("Loss became NaN, stopping training.")
        if loss.ndim > 0:
            loss = loss.mean()
        self.loss = loss
        
        # ç»Ÿè®¡ Running Loss
        batch_size = self.target.size(0) if hasattr(self.target, 'size') else 1
        loss_scalar = loss.item()
        self.running_loss += loss_scalar * batch_size
        self.running_samples += batch_size

        # è‡ªåŠ¨è®°å½• TensorBoard (Batchçº§)
        if self.writer is not None:
            self.writer.add_scalar('Train/Batch_Loss', loss_scalar, self.global_step)
            # è®°å½•å­¦ä¹ ç‡ (å–ç¬¬ä¸€ä¸ª param_group)
            current_lr = self.optimizer.param_groups[0]['lr']
            self.writer.add_scalar('Train/LR', current_lr, self.global_step)

        # Backward & Update
        self.update(loss, step_plateau_with_train_loss)
        return loss

    def auto_step_scheduler(self, loss_val: Optional[torch.Tensor] = None, use_train_loss: bool = False) -> None:
        """
        è¾…åŠ©æ–¹æ³•ï¼šæ ¹æ®è°ƒåº¦å™¨ç±»å‹è‡ªåŠ¨æ‰§è¡Œ stepã€‚
        ä¸»è¦ç”¨äºå¤„ç† ReduceLROnPlateau éœ€è¦ metric çš„æƒ…å†µã€‚
        """
        if self.scheduler is None:
            return

        # å¦‚æœæ˜¯ Plateau è°ƒåº¦å™¨ï¼Œé€šå¸¸åœ¨ Epoch ç»“æŸæ—¶è°ƒç”¨ï¼Œä½†å¦‚æœç”¨æˆ·å¸Œæœ›åŸºäº batch loss ä¹Ÿå¯ä»¥
        if isinstance(self.scheduler, ReduceLROnPlateau):
            if use_train_loss and loss_val is not None and self.is_last_batch_in_epoch:
                self.scheduler.step(loss_val.item())
        # å¦‚æœæ˜¯ OneCycleLR æˆ–å…¶ä»–éœ€è¦æ¯ä¸ª batch step çš„è°ƒåº¦å™¨
        elif hasattr(self.scheduler, 'step_batch'): # è‡ªå®šä¹‰å±æ€§æ ‡è®°æˆ–æ£€æŸ¥ç±»å‹
             pass # é€šå¸¸ç”±å¤–éƒ¨æ˜¾å¼è°ƒç”¨ï¼Œæˆ–è€…åœ¨è¿™é‡Œæ·»åŠ é€»è¾‘

    def calculate_classification_metrics(self) -> float:
        """
        è®¡ç®—å¸¸è§„åˆ†ç±»ä»»åŠ¡çš„ Loss å’Œ Accuracyã€‚
        æ›´æ–° eval_loss å’Œ correct_predictionsã€‚
        å¦‚æœè°ƒç”¨äº† init_classesï¼Œè¿˜ä¼šç´¯ç§¯é¢„æµ‹ç»“æœå¹¶åœ¨ epoch ç»“æŸæ—¶ç»˜åˆ¶æ··æ·†çŸ©é˜µã€‚

        Returns:
            float: å½“å‰ Batch çš„ Lossã€‚
        """
        with autocast(device_type=self.device.type, enabled=self.use_amp):
            if isinstance(self.data, tuple):
                logits = self.model(*self.data)
            else:
                logits = self.model(self.data)
            
            # å¤„ç† shape [N,1] vs [N]
            logits_squeezed = match_shape_if_needed(logits, self.target)
            loss = self.criterion(logits_squeezed, self.target) if self.criterion else torch.tensor(0.0)

        batch_size = self.target.size(0)
        self.eval_loss += loss.item() * batch_size
        self.total_predictions += batch_size
        
        # è®¡ç®— Acc å¹¶å‡†å¤‡æ··æ·†çŸ©é˜µæ•°æ®
        preds = None
        targets = None
        scores = None # ç”¨äº ROC

        # å¤šåˆ†ç±» (Logits shape [N, C], C > 1)
        if logits.ndim > 1 and logits.shape[1] > 1:
            preds = logits.argmax(dim=1)
            scores = torch.softmax(logits, dim=1) # æ¦‚ç‡
            if self.target.ndim > 1: # target æ˜¯ one-hot æˆ– probabilities
                targets = self.target.argmax(dim=1)
            else: # target æ˜¯ indices
                targets = self.target
            self.correct_predictions += (preds == targets).sum().item()
        # äºŒåˆ†ç±» (Logits shape [N, 1] æˆ– [N])
        else:
            # å‡è®¾ logits ä¸º raw scoreï¼Œåº”ç”¨ sigmoid
            if logits_squeezed.ndim == 0: # scalar
                 scores = torch.sigmoid(logits_squeezed)
                 preds = (scores > 0.5).float()
            else:
                 scores = torch.sigmoid(logits_squeezed)
                 preds = (scores > 0.5).float()
            targets = self.target
            self.correct_predictions += (preds == targets).sum().item()
        
        # è®¡ç®— Top-k Accuracy
        if self.top_k is not None and logits.ndim > 1 and logits.shape[1] >= self.top_k:
            # logits: [N, C], target: [N]
            _, pred_topk = logits.topk(self.top_k, dim=1, largest=True, sorted=True) # [N, k]
            pred_topk = pred_topk.t() # [k, N]
            correct = pred_topk.eq(targets.view(1, -1).expand_as(pred_topk))
            self.correct_top_k_predictions += correct.reshape(-1).float().sum().item()

        # ç´¯ç§¯æ··æ·†çŸ©é˜µå’Œ ROC æ•°æ®
        if self.classes is not None:
            if self.enable_confusion_matrix:
                self.y_preds.append(preds.detach().cpu())
                self.y_trues.append(targets.detach().cpu())
            
            if self.enable_roc_curve:
                # ç¡®ä¿ y_trues ä¹Ÿè¢«æ”¶é›† (å¦‚æœä¸Šé¢æ²¡æ”¶é›†)
                if not self.enable_confusion_matrix:
                    self.y_trues.append(targets.detach().cpu())
                self.y_scores.append(scores.detach().cpu())

            # å¦‚æœæ˜¯æœ€åä¸€ä¸ª batchï¼Œç”Ÿæˆå›¾è¡¨
            if self.is_last_batch_in_epoch:
                all_trues = torch.cat(self.y_trues).numpy()
                
                # 1. æ··æ·†çŸ©é˜µ
                if self.enable_confusion_matrix:
                    all_preds = torch.cat(self.y_preds).numpy()
                    fig_cm = plot_confusion_matrix(all_trues, all_preds, self.classes)
                    if self.writer is not None:
                        self.writer.add_figure("Eval/Confusion_Matrix", fig_cm, self.global_step)
                    else:
                        save_path = f"confusion_matrix_epoch_{self.display_epoch}.png"
                        fig_cm.savefig(save_path)
                        print(f"Confusion matrix saved to {save_path}")
                    plt.close(fig_cm)

                    # æ‰“å° Per-Class Accuracy
                    cm = confusion_matrix(all_trues, all_preds)
                    with np.errstate(divide='ignore', invalid='ignore'):
                        per_class_acc = cm.diagonal() / cm.sum(axis=1)
                        per_class_acc = np.nan_to_num(per_class_acc)
                    
                    print("-" * 40)
                    print(f"{'Class':<15} | {'Accuracy':<10}")
                    print("-" * 40)
                    for i, acc in enumerate(per_class_acc):
                        if i < len(self.classes):
                            class_name = str(self.classes[i])
                            print(f"Accuracy of {class_name:<15} : {100 * acc:.2f}%")
                    print("-" * 40)
                
                # 2. ROC æ›²çº¿
                if self.enable_roc_curve:
                    all_scores = torch.cat(self.y_scores).numpy()
                    fig_roc = plot_roc_curve(all_trues, all_scores, self.classes)
                    if self.writer is not None:
                        self.writer.add_figure("Eval/ROC_Curve", fig_roc, self.global_step)
                    else:
                        save_path = f"roc_curve_epoch_{self.display_epoch}.png"
                        fig_roc.savefig(save_path)
                        print(f"ROC curve saved to {save_path}")
                    plt.close(fig_roc)

                # 3. æ‰“å° Top-k Accuracy
                if self.top_k is not None:
                    top_k_acc = self.correct_top_k_predictions / self.total_predictions
                    print(f"Top-{self.top_k} Accuracy: {100 * top_k_acc:.2f}%")

                # æ¸…ç©ºåˆ—è¡¨ä»¥å¤‡ä¸‹ä¸€æ¬¡è¯„ä¼°
                self.y_preds = []
                self.y_trues = []
                self.y_scores = []

        return loss.item()

    def record_history(self, current_val_loss: float = None, current_val_acc: float = None):
        """
        æ‰‹åŠ¨å°†éªŒè¯é›†æŒ‡æ ‡æ·»åŠ åˆ° history å­—å…¸ä¸­ã€‚
        """
        if current_val_loss is not None:
            self.history['val_loss'].append(current_val_loss)
        if current_val_acc is not None:
            self.history['val_acc'].append(current_val_acc)

    def predict(self, data_loader: DataLoader, return_targets: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        æ‰§è¡Œæ¨ç†å¹¶è¿”å›æ‰€æœ‰æ ·æœ¬çš„é¢„æµ‹ç»“æœã€‚
        
        Args:
            data_loader: æ¨ç†æ•°æ®åŠ è½½å™¨ã€‚
            return_targets: æ˜¯å¦åŒæ—¶ä¹Ÿè¿”å›æ ‡ç­¾ (ç”¨äºè®¡ç®—æ··æ·†çŸ©é˜µç­‰)ã€‚
            
        Returns:
            predictions (Tensor): æ‹¼æ¥åçš„é¢„æµ‹ç»“æœ (CPU Tensor)ã€‚
            targets (Tensor, optional): æ‹¼æ¥åçš„æ ‡ç­¾ (CPU Tensor)ã€‚
        """
        self.model.eval()
        # å¦‚æœä½¿ç”¨äº† EMAï¼Œå»ºè®®åœ¨æ¨ç†æ—¶ä½¿ç”¨ EMA çš„æƒé‡ (å¯é€‰ï¼Œè¿™é‡Œæš‚ä¸å¼ºåˆ¶è¦†ç›–ï¼Œä»¥å…å½±å“åç»­è®­ç»ƒ)
        # ä½ å¯ä»¥æ‰‹åŠ¨è°ƒç”¨ trainer.ema.apply_shadow(trainer.model) æ¥åº”ç”¨
        
        preds_list = []
        targets_list = []
        
        print(f"Predicting on {len(data_loader.dataset)} samples...")
        try:
            with torch.no_grad():
                for batch_data in tqdm(data_loader, desc="Predicting", leave=False):
                    self._process_batch_data(batch_data)
                    
                    # Forward
                    with autocast(device_type=self.device.type, enabled=self.use_amp):
                        if isinstance(self.data, tuple):
                            logits = self.model(*self.data)
                        else:
                            logits = self.model(self.data)
                    
                    # ç§»åŠ¨åˆ° CPU ä»¥é˜²æ˜¾å­˜æº¢å‡º
                    preds_list.append(logits.detach().cpu())
                    
                    if return_targets and self.target is not None:
                        targets_list.append(self.target.detach().cpu())
                        
        finally:
            self.model.train() # æ¢å¤è®­ç»ƒæ¨¡å¼

        if len(preds_list) == 0:
            return torch.tensor([])

        predictions = torch.cat(preds_list, dim=0)
        
        if return_targets and len(targets_list) > 0:
            targets = torch.cat(targets_list, dim=0)
            return predictions, targets
            
        return predictions

    def auto_checkpoint(self, metrics: Optional[Dict[str, float]] = None, save_best_only: bool = False, monitor: str = 'val_acc') -> None:
        """
        è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ã€‚
        
        åœ¨ Epoch ç»“æŸæ—¶è°ƒç”¨ã€‚ä¼šä¿å­˜ 'last.pt'ã€‚
        å¦‚æœæä¾›äº† metrics ä¸” monitor æŒ‡æ ‡ä¼˜äºå†å²æœ€ä½³ï¼Œåˆ™ä¿å­˜ 'best_model.pt'ã€‚

        Args:
            metrics (Dict): å½“å‰ Epoch çš„è¯„ä¼°æŒ‡æ ‡å­—å…¸ã€‚
            save_best_only (bool): è¿™é‡Œçš„é€»è¾‘é€šå¸¸æ˜¯åªä¿ç•™ bestï¼Œä½†æœ¬å‡½æ•°ä¼šåŒæ—¶ä¿ç•™ lastã€‚
            monitor (str): ç›‘æ§çš„æŒ‡æ ‡ keyï¼Œç”¨äºåˆ¤æ–­æœ€ä½³æ¨¡å‹ã€‚
        """
        if not self.is_last_batch_in_epoch or not self.checkpoint_path:
            return
        
        # 1. ä¿å­˜å½“å‰æœ€æ–°çŠ¶æ€
        self.save_checkpoint(extra_info=metrics) # é»˜è®¤ä¿å­˜åˆ° self.checkpoint_path
        
        # 2. åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        is_best = False
        if metrics and monitor in metrics:
            current_val = metrics[monitor]
            
            # åˆå§‹åŒ– best metric
            if self.best_val_metric == -float('inf') and 'loss' in monitor:
                 self.best_val_metric = float('inf')

            if 'loss' in monitor:
                if current_val < self.best_val_metric:
                    self.best_val_metric = current_val
                    is_best = True
            else:
                if current_val > self.best_val_metric:
                    self.best_val_metric = current_val
                    is_best = True
        
        if is_best:
            best_path = os.path.join(os.path.dirname(self.checkpoint_path), 'best_model.pt')
            self.save_checkpoint(path=best_path, extra_info=metrics)
            print(f" -> New best model saved at epoch {self.display_epoch} ({monitor}: {metrics[monitor]:.4f})")
    
    def fit(
            self,
            train_loader: Optional[DataLoader] = None,
            test_loader: Optional[DataLoader] = None,
            cal_classification_metrics: bool = False,
        ) -> 'Trainer':
        """
        å‚»ç“œå¼è®­ç»ƒå™¨ã€‚
        æ”¯æŒè‡ªåŠ¨æ›´æ–°ã€è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ã€è‡ªåŠ¨è®¡ç®—åˆ†ç±»æŒ‡æ ‡ã€‚
        
        Args:
            train_loader (Optional[DataLoader]): è®­ç»ƒæ•°æ®åŠ è½½å™¨ã€‚
            test_loader (Optional[DataLoader]): æµ‹è¯•æ•°æ®åŠ è½½å™¨ã€‚
            cal_classification_metrics (bool): æ˜¯å¦è®¡ç®—åˆ†ç±»æŒ‡æ ‡ã€‚
            
        Returns:
            self (Trainer): è®­ç»ƒå™¨å¯¹è±¡ï¼Œç”¨äºé“¾å¼è°ƒç”¨ã€‚
        """
        if not self.model:
            raise RuntimeError("Model missing.")
        if not self.optimizer or not self.criterion:
            raise RuntimeError("Optimizer or Criterion missing.")
        for trainer in self.train(train_loader, tqdm_bar=True, print_loss=True):
            trainer.auto_update()
            trainer.auto_checkpoint()
        
        if cal_classification_metrics:
            for trainer in self.eval(test_loader, tqdm_bar=True):
                trainer.calculate_classification_metrics()
            print(f'Mean Accuracy: {100 * self.eval_accuracy:.2f}%')
        
        return self

    def save_checkpoint(self, path: Optional[str] = None, extra_info: Optional[Dict[str, Any]] = None) -> None:
        path_to_use = path if path is not None else self.checkpoint_path
        if path_to_use is None: return
        
        os.makedirs(os.path.dirname(path_to_use), exist_ok=True)

        # è·å–åŸå§‹æ¨¡å‹ state_dict
        if isinstance(self.model, nn.DataParallel):
            model_state = self.model.module.state_dict()
        else:
            model_state = self.model.state_dict()
        
        state = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'ema_state_dict': self.ema.shadow.state_dict() if (self.use_ema and self.ema) else None,
            
            'history': self.history,
            'best_val_metric': self.best_val_metric,
            'patience_counter': self.patience_counter,
            'best_metric_for_es': self.best_metric_for_es
        }
        if extra_info:
            state.update(extra_info)
        
        try:
            torch.save(state, path_to_use)
        except Exception as e:
            print(f"Error saving checkpoint {path_to_use}: {e}")

    def load_checkpoint(self, path: Optional[str] = None) -> 'Trainer':
        path_to_use = path if path is not None else self.checkpoint_path
        if path_to_use is None or not os.path.exists(path_to_use):
            return self

        print(f"Loading checkpoint: {path_to_use}")
        try:
            checkpoint = torch.load(path_to_use, map_location=self.device)
            
            # åŠ è½½æƒé‡
            if isinstance(self.model, nn.DataParallel):
                # å¦‚æœå½“å‰æ˜¯å¤šå¡ï¼ŒåŠ è½½åˆ° model.module
                self.model.module.load_state_dict(checkpoint['model_state_dict'])
            else:
                # å¦‚æœå½“å‰æ˜¯å•å¡ï¼Œç›´æ¥åŠ è½½
                self.model.load_state_dict(checkpoint['model_state_dict'])
            
            if self.optimizer and checkpoint.get('optimizer_state_dict'):
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if self.scheduler and checkpoint.get('scheduler_state_dict'):
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            if self.scaler and checkpoint.get('scaler_state_dict'):
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
            if self.use_ema and self.ema and checkpoint.get('ema_state_dict'):
                self.ema.shadow.load_state_dict(checkpoint['ema_state_dict'])
                print("EMA state loaded.")

            self.start_epoch = checkpoint.get('epoch', -1) + 1
            self.global_step = checkpoint.get('global_step', 0)
            self.history = checkpoint.get('history', self.history)
            self.best_val_metric = checkpoint.get('best_val_metric', -float('inf'))
            self.patience_counter = checkpoint.get('patience_counter', 0)
            self.best_metric_for_es = checkpoint.get('best_metric_for_es', None)
            
            print(f"Resumed from Epoch {self.display_epoch - 1} (Global Step: {self.global_step}).")
        except Exception as e:
            print(f"Failed to load checkpoint: {e}. Starting from scratch.")
            self.start_epoch = 0
        
        return self
    
    def save_model(self, path: str) -> None:
        """
        ä»…ä¿å­˜æ¨¡å‹çš„æƒé‡å‚æ•° (state_dict)ï¼Œç”¨äºæ¨ç†éƒ¨ç½²ã€‚
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        try:
            if isinstance(self.model, nn.DataParallel):
                torch.save(self.model.module.state_dict(), path)
            else:
                torch.save(self.model.state_dict(), path)
            print(f"Model weights saved to: {path}")
        except Exception as e:
            print(f"Error saving model weights to {path}: {e}")

    def load_model(self, path: str, strict: bool = True) -> None:
        """
        åŠ è½½æ¨¡å‹æƒé‡ã€‚è‡ªåŠ¨å¤„ç†â€œçº¯æƒé‡æ–‡ä»¶â€å’Œâ€œå®Œæ•´æ£€æŸ¥ç‚¹æ–‡ä»¶â€ã€‚
        """
        if not os.path.exists(path):
            print(f"Error: Model file not found at {path}")
            return

        print(f"Loading model weights from: {path}")
        try:
            state_dict = torch.load(path, map_location=self.device)
            
            # å…¼å®¹å®Œæ•´ checkpoint æ–‡ä»¶
            if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:
                print("Detected full checkpoint, extracting 'model_state_dict'...")
                state_dict = state_dict['model_state_dict']
            
            # å…¼å®¹å¤„ç† DataParallel åŠ è½½
            if isinstance(self.model, nn.DataParallel):
                missing, unexpected = self.model.module.load_state_dict(state_dict, strict=strict)
            else:
                missing, unexpected = self.model.load_state_dict(state_dict, strict=strict)
            
            if not strict:
                if missing: print(f"Missing keys (ignored): {len(missing)}")
                if unexpected: print(f"Unexpected keys (ignored): {len(unexpected)}")
            
            print("Model weights loaded successfully.")
            
        except Exception as e:
            print(f"Failed to load model weights: {e}")
